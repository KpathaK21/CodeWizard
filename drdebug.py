import argparse
from llm.openai_llm import OpenAIModel  # Claude can be added later

def get_llm(name):
    if name.startswith("openai:"):
        return OpenAIModel(name.split(":")[1])
    else:
        raise ValueError("Unsupported LLM model")

def main():
    parser = argparse.ArgumentParser(description="Dr.Debug â€“ AI Debugging Assistant")
    parser.add_argument("--llm", type=str, default="openai:gpt-4o",
                        help="Choose LLM provider and model (e.g. openai:gpt-4o)")
    args = parser.parse_args()

    print("\nðŸ› Welcome to Dr.Debug! Paste your error message, stack trace, or buggy code below.")
    user_input = input(">>> ")

    llm = get_llm(args.llm)

    while True:
        print("\nðŸ§  Thinking...\n")
        explanation = llm.generate(user_input)
        print("\nðŸ’¡ Suggestion:\n")
        print(explanation)

        next_step = input("\nâ“ Want to ask a follow-up or add more context? (y/n): ").strip().lower()
        if next_step != 'y':
            print("\nðŸ‘‹ Exiting. Good luck fixing that bug!")
            break

        user_input = input("\nðŸ“ Add more details or follow-up question:\n>>> ")

if __name__ == "__main__":
    main()
